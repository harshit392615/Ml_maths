{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSm+QwqnNcK+r0bp6jaezj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Loss Function & Optimization"],"metadata":{"id":"fN2JR85vQNC7"}},{"cell_type":"code","source":["import numpy as np\n","x_train = np.array([1.2, 2.0, 2.8, 3.6, 4.5, 5.1, 6.0, 6.7])\n","y_train = np.array([8.5, 11.1, 13.9, 16.2, 19.8, 21.4, 23.9, 26.1])\n","\n","x_test = np.array([7.5, 8.2, 9.0, 9.8])\n","y_test = np.array([28.4, 30.1, 32.7, 34.6])"],"metadata":{"id":"CJ40b3cCRg5Y","executionInfo":{"status":"ok","timestamp":1766663297225,"user_tz":-330,"elapsed":17,"user":{"displayName":"Harshit","userId":"12316949519422484610"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["Mean Squared Error (MSE) ‚Äî Standard Loss Function"],"metadata":{"id":"7jbt6F27Qjhw"}},{"cell_type":"code","source":["def mse(y_t , y_p ):\n","  sum = 0\n","  for i in range(0,len(y_t)):\n","    sum += (y_t[i] - y_p[i])**2\n","  return sum / len(y_t)"],"metadata":{"id":"n-8psocOQVlW","executionInfo":{"status":"ok","timestamp":1766663362347,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harshit","userId":"12316949519422484610"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["slope = 3.219369003866164\n","constant = 4.77526609708367\n","\n","y_pred = x_train*slope + constant\n","mse_train = mse(y_train , y_pred)\n","print(mse_train)\n","\n","y_pred = x_test*slope + constant\n","mse_test = mse(y_test , y_pred)\n","print(mse_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J__GIGjpUCJo","executionInfo":{"status":"ok","timestamp":1766663363922,"user_tz":-330,"elapsed":19,"user":{"displayName":"Harshit","userId":"12316949519422484610"}},"outputId":"fa035647-882e-4d8e-a85d-99aa86e85f17"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0.062465634098611125\n","1.3755427343006463\n"]}]},{"cell_type":"markdown","source":["Cost Function\n"],"metadata":{"id":"qqnuCE7GXFw0"}},{"cell_type":"markdown","source":["A cost function measures how wrong the model is over the entire dataset.\n","While a loss function measures error for one data point, the cost function aggregates those losses across all data points."],"metadata":{"id":"9zoKdto2XPSb"}},{"cell_type":"markdown","source":["mse is a example of cost function our goal is to minimize cost function"],"metadata":{"id":"48DLIuoAXR9N"}},{"cell_type":"markdown","source":["Closed-Form Solution (Normal Equation)\n","\n","ùúÉ = (X'X)^-1X'y\n","\n","Where:\n","\n","ùëã\n"," = feature matrix (with bias column)\n","\n","ùúÉ\n"," = parameters\n","[\n","ùëè\n",",\n","ùë§\n","]\n","[b,w]\n","\n","Pros\n","\n","1.Exact solution\n","\n","2.No iterations\n","\n","Cons\n","\n","1.Matrix inversion is expensive\n","\n","2.Fails if\n","ùëã'ùëã is non-invertible\n","\n","3.Not scalable for large datasets"],"metadata":{"id":"1wVaiIowX8x3"}},{"cell_type":"code","source":["ones = np.ones(len(x_train))\n","X = np.column_stack((ones , x_train))\n","\n","XT = X.T\n","XTX = XT@X\n","\n","XTX_inv = np.linalg.inv(XTX)\n","theta = XTX_inv @ XT @ y_train\n","\n","b = theta[0]\n","w = theta[1]\n","\n","print(b)\n","print(w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OyOO9kGXGcG","executionInfo":{"status":"ok","timestamp":1766665098852,"user_tz":-330,"elapsed":23,"user":{"displayName":"Harshit","userId":"12316949519422484610"}},"outputId":"f1e1125f-ba3f-471d-b6d1-1abdb23b7d67"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["4.775266097083676\n","3.2193690038661646\n"]}]}]}